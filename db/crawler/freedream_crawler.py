import requests
from bs4 import BeautifulSoup
import chardet
import time
import json
import os
from urllib.parse import quote

# âœ… í¬ë¡¤ë§ í•¨ìˆ˜: "ê¿ˆ" ìœ¼ë¡œ ëê¹Œì§€
def crawl_all_from_keyword(keyword="ê¿ˆ"):
    result = []
    encoded_keyword = quote(keyword, encoding='euc-kr')
    page = 1

    while True:
        print(f"[{page}] ğŸ“„ [{keyword}] í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘...")

        url = (
            f"http://www.freedream24.com/freedream/dreamsearch.asp"
            f"?page={page}"
            f"&keyword={encoded_keyword}"
            f"&result_in="
            f"&keyword_table={encoded_keyword}"
            f"&keyword_count=0"
            f"&scheck=no"
        )

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ [{page}]: {e}")
            break

        detected_encoding = chardet.detect(response.content)
        html_content = response.content.decode(detected_encoding['encoding'], errors='replace')
        soup = BeautifulSoup(html_content, 'html.parser')
        raw_text = soup.get_text(separator='\n').strip()
        lines = raw_text.split('\n')

        found_any = False
        temp_title = None
        next_is_description = False

        for line in lines:
            line = line.strip()

            if line.endswith('ê¿ˆ') and 'ê¿ˆì˜ ë²ˆí˜¸' not in line:
                temp_title = line
                continue

            if line == 'ê¿ˆì˜ ë²ˆí˜¸' and temp_title:
                next_is_description = True
                continue

            if next_is_description and line == '':
                continue

            if next_is_description and line != '':
                interpretation = line
                result.append({
                    'dream': temp_title,
                    'interpretation': interpretation
                })
                temp_title = None
                next_is_description = False
                found_any = True

        if not found_any:
            print(f"ğŸ›‘ [{page}] ë” ì´ìƒ ë°ì´í„° ì—†ìŒ. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        page += 1
        time.sleep(1)

    return result

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    os.makedirs('data', exist_ok=True)

    print("ğŸš€ ì „ì²´ í•´ëª½ DB í¬ë¡¤ë§ ì‹œì‘ (ê²€ìƒ‰ì–´: ê¿ˆ)...")
    all_dreams = crawl_all_from_keyword("ê¿ˆ")

    save_path = "data/dream_data_all.json"
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(all_dreams, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì´ ìˆ˜ì§‘ëœ ê¿ˆ ê°œìˆ˜: {len(all_dreams)}")
    print(f"ğŸ“ ì €ì¥ ì™„ë£Œ: {save_path}")
